{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "import nltk\r\n",
    "# 1: +ve, 0: -ve"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "import json\r\n",
    "\r\n",
    "with open('dataset/strong-negatives.json') as fopen:\r\n",
    "    myfile1 = json.load(fopen)\r\n",
    "\r\n",
    "with open('dataset/strong-positives.json') as fopen:\r\n",
    "    myfile2 = json.load(fopen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "negative_data = pd.DataFrame(myfile1, columns = ['text'])\r\n",
    "# negative_data = negative_data[:100000]\r\n",
    "negative_data['label'] = 0\r\n",
    "print(negative_data.head(10))\r\n",
    "print(len(negative_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                text  label\n",
      "0  @andcrra Ngelamar kasih cincin tp kok mukanya ...      0\n",
      "1  @mojokdotco Caption iki nggarai uwong males ni...      0\n",
      "2  Valentino Rossi Tidak Setuju Kompetisi MotoGP ...      0\n",
      "3  Loop in nama dlm email pon boleh jd issue... D...      0\n",
      "4                             Hilang nyawaku aku tgk      0\n",
      "5             @AyekKamal yer lah sbb sombong mmg lah      0\n",
      "6  Guys, tolong rt tweet ni sampai owner dia dapa...      0\n",
      "7  Aku ada motor racing ,\\naku bawa ronda ,\\nawek...      0\n",
      "8  - STILL 17 -\\nSEDIHBGT!!!! Kebayang kan betapa...      0\n",
      "9  @jokowi Bangga manfaat \"DILAN\" perputaran uang...      0\n",
      "1892193\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "positive_data = pd.DataFrame(myfile2, columns = ['text'])\r\n",
    "# positive_data = positive_data[:100000]\r\n",
    "positive_data['label'] = 1\r\n",
    "print(positive_data.head(10))\r\n",
    "print(len(positive_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                text  label\n",
      "0  @nasikebuli07 @FANBASEBOKEP2 Dom jakpus sih, b...      1\n",
      "1                  @seolasidooo Jujur kacang ijo !!       1\n",
      "2            Sahur tengah malam kaya nya enak ya...       1\n",
      "3  @Ini_Talkshow @tiket\\n\\nMakan serabi enak pas ...      1\n",
      "4  @imtaeyonglee loh kenapa? kan marga oppa juga ...      1\n",
      "5  Resort Datuk Jhon Gani..kuala penyu..boleh baw...      1\n",
      "6  |190506| BTS at Rose Bowl Day 1|\\n\\n Jadi mula...      1\n",
      "7  I'm at Menara Axis in Petaling Jaya, Selangor ...      1\n",
      "8  Usually aku rimas tngok parking kiri kanan\\n\\n...      1\n",
      "9  Perlu hijrah sebentar dari sini untuk menenang...      1\n",
      "1084592\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "news_sentiment = pd.read_csv('./dataset/sentiment-data-v2.csv')\r\n",
    "news_sentiment.loc[news_sentiment['label'] == 'Positive', 'label'] = 1\r\n",
    "news_sentiment.loc[news_sentiment['label'] == 'Negative', 'label'] = 0\r\n",
    "news_sentiment['label'] = news_sentiment['label'].astype('int64')\r\n",
    "news_sentiment = news_sentiment[['text', 'label']]\r\n",
    "print(news_sentiment.head(10))\r\n",
    "print(len(news_sentiment))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                text  label\n",
      "0  Lebih-lebih lagi denganÂ  kemudahan internet da...      0\n",
      "1  boleh memberi teguran kepada parti tetapi perl...      1\n",
      "2  Adalah membingungkan mengapa masyarakat Cina b...      0\n",
      "3  Kami menurunkan defisit daripada 6.7 peratus p...      1\n",
      "4        Ini masalahnya. Bukan rakyat, tetapi sistem      0\n",
      "5  Masyarakat Cina dapat melihat bagaimana peranc...      1\n",
      "6  Tetapi penyelenggaraan ini ada skop tertentu y...      1\n",
      "7  Jika memilih untuk tidak dikritik, duduk saja ...      1\n",
      "8  Ini membawa erti haram juga untuk sokong BN ke...      0\n",
      "9  Jika anda tidak percaya, tanya ibu bapa di Pul...      1\n",
      "3685\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# Combine both dataframes into one master dataframe\r\n",
    "data = pd.concat([negative_data, positive_data, news_sentiment], ignore_index = True)\r\n",
    "print(data.head(10))\r\n",
    "print(len(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                text  label\n",
      "0  @andcrra Ngelamar kasih cincin tp kok mukanya ...      0\n",
      "1  @mojokdotco Caption iki nggarai uwong males ni...      0\n",
      "2  Valentino Rossi Tidak Setuju Kompetisi MotoGP ...      0\n",
      "3  Loop in nama dlm email pon boleh jd issue... D...      0\n",
      "4                             Hilang nyawaku aku tgk      0\n",
      "5             @AyekKamal yer lah sbb sombong mmg lah      0\n",
      "6  Guys, tolong rt tweet ni sampai owner dia dapa...      0\n",
      "7  Aku ada motor racing ,\\naku bawa ronda ,\\nawek...      0\n",
      "8  - STILL 17 -\\nSEDIHBGT!!!! Kebayang kan betapa...      0\n",
      "9  @jokowi Bangga manfaat \"DILAN\" perputaran uang...      0\n",
      "2980470\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "# save data in csv file\r\n",
    "# data.to_csv('bahasa_sentiment.csv', encoding='utf-8',index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "data['label'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1893488\n",
       "1    1086982\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "# handle missing values\r\n",
    "data = data.dropna(axis = 0, how ='any')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import re\r\n",
    "import stopwordsiso as sw\r\n",
    "\r\n",
    "# download required library from nltk\r\n",
    "# nltk.download('punkt')\r\n",
    "\r\n",
    "# create stemmer for bahasa\r\n",
    "factory = StemmerFactory()\r\n",
    "stemmer = factory.create_stemmer()\r\n",
    "\r\n",
    "# stop words consist of malay, indo, english\r\n",
    "stop_words_main = list(sw.stopwords([\"ms\", \"id\", \"en\"]))\r\n",
    "# custom stopwords such as shortform\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words_main+stop_words_custom)\r\n",
    "\r\n",
    "def remove_emoji(text):\r\n",
    "    emoji_pattern = re.compile(\"[\"\r\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\r\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\r\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\r\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\r\n",
    "                           u\"\\U00002702-\\U000027B0\"\r\n",
    "                           u\"\\U000024C2-\\U0001F251\"\r\n",
    "                           \"]+\", flags=re.UNICODE)\r\n",
    "    return emoji_pattern.sub(r'', text)\r\n",
    "\r\n",
    "def text_preprocessing(text):\r\n",
    "\r\n",
    "    # remove numbers\r\n",
    "    text = re.sub(r'\\d+', '', text)\r\n",
    "\r\n",
    "    # remove links\r\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "\r\n",
    "    # remove word with tweethandle @name\r\n",
    "    text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "\r\n",
    "    # remove emoji\r\n",
    "    text = remove_emoji(text)\r\n",
    "\r\n",
    "    # tokennization\r\n",
    "    tokens = word_tokenize(text)\r\n",
    "\r\n",
    "    # stemmer and remove punctuation\r\n",
    "    words = []\r\n",
    "    for token in tokens:\r\n",
    "        if token not in string.punctuation:\r\n",
    "            temp = stemmer.stem(token)\r\n",
    "            words.append(temp)\r\n",
    "\r\n",
    "    # remove stopwords\r\n",
    "    cleaned = []\r\n",
    "    for word in words:\r\n",
    "        if word not in STOP_WORDS:\r\n",
    "            cleaned.append(word)\r\n",
    "\r\n",
    "    # join all words into a complete sentence \r\n",
    "    complete_sentence = ' '.join([str(word) for word in cleaned])\r\n",
    "\r\n",
    "    # remove extra line spaces between words in a sentence\r\n",
    "    complete_sentence = \" \".join(complete_sentence.split())\r\n",
    "    \r\n",
    "    return complete_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "sample = '@natasya Aku pernah first day raya tudung aku terbakar ðŸ˜©ðŸ˜©ðŸ˜©.... nasib baik tak moody sebab kecik je bekas bakar. https://t.co/24DGRbL70Z'\r\n",
    "text_preprocessing(sample)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'day tudung bakar nasib moody kecik bakar'"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "text_cleaning = lambda x: text_preprocessing(x)\r\n",
    "data['cleaned_Text'] = pd.DataFrame(data['text'].apply(text_cleaning))\r\n",
    "data['cleaned_Text'].head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    ngelamar kasih cincin muka songong sedih gue liat\n",
       "1    iki nggarai uwong males nikah min manusia arep...\n",
       "2          valentino rossi tuju kompetisi motogp eropa\n",
       "3    loop nama dlm email pon jd issue email reply j...\n",
       "4                                     hilang nyawa tgk\n",
       "5                                  yer sbb sombong mmg\n",
       "6    guys tolong tweet owner phone tinggal teks pak...\n",
       "7    motor racing bawa ronda awek bonceng dar dar l...\n",
       "8       sedihbgt bayang betapa sedih gimana thn seteru\n",
       "9    bangga manfaat dil putar uang dukung tumbuh mi...\n",
       "Name: cleaned_Text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# SPLIT TRAINING & TESTING DATA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_Text'],data['label'],test_size=0.2,shuffle=True, random_state=42)\r\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2384376,) (2384376,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.svm import LinearSVC \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_pipeline(data_train_input,data_train_target,model_type):\r\n",
    "    # Classifier selection\r\n",
    "    if model_type == \"linear\":\r\n",
    "        classifier = LinearSVC()\r\n",
    "    elif model_type == \"logistic\":\r\n",
    "        classifier = LogisticRegression(max_iter=1000)\r\n",
    "    elif model_type == \"sgd\":\r\n",
    "        classifier = SGDClassifier()\r\n",
    "    elif model_type == \"naive_bayes\":\r\n",
    "        classifier = MultinomialNB()\r\n",
    "    elif model_type == \"xgboost\":\r\n",
    "        classifier = XGBClassifier(use_label_encoder=False,eta=0.1,gamma=0.3, n_estimators=100, learning_rate=0.5, min_child_weight=5, \r\n",
    "        max_depth=5, colsample_bytree=0.7,objective=\"binary:logistic\", eval_metric=\"logloss\",verbosity=0)\r\n",
    "\r\n",
    "    tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "    # Pipeline setup\r\n",
    "    clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\r\n",
    "\r\n",
    "    model = clf.fit(data_train_input,data_train_target)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# Support Vector Classification\r\n",
    "svm_model = sentiment_pipeline(X_train, y_train, 'linear')\r\n",
    "sentiment_model_predict(svm_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 94.7\n",
      "Precision : 94.04\n",
      "Recall : 94.66\n",
      "F1-Score :94.33\n",
      "[[358735  19618]\n",
      " [ 11949 205792]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "# Logistic Regression\r\n",
    "lr_model = sentiment_pipeline(X_train, y_train, 'logistic')\r\n",
    "sentiment_model_predict(lr_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 94.75\n",
      "Precision : 94.11\n",
      "Recall : 94.67\n",
      "F1-Score :94.38\n",
      "[[359334  19019]\n",
      " [ 12269 205472]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# Stochastic Gradient Descent\r\n",
    "sgd_model = sentiment_pipeline(X_train, y_train, 'sgd')\r\n",
    "sentiment_model_predict(sgd_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 90.64\n",
      "Precision : 90.9\n",
      "Recall : 88.76\n",
      "F1-Score :89.65\n",
      "[[362137  16216]\n",
      " [ 39603 178138]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# Multinomial Naive Bayes\r\n",
    "nb_model = sentiment_pipeline(X_train, y_train, 'naive_bayes')\r\n",
    "sentiment_model_predict(nb_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 87.96\n",
      "Precision : 89.67\n",
      "Recall : 84.56\n",
      "F1-Score :86.23\n",
      "[[367618  10735]\n",
      " [ 61064 156677]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# Xgboost\r\n",
    "xg_model = sentiment_pipeline(X_train, y_train, 'xgboost')\r\n",
    "sentiment_model_predict(xg_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 84.03\n",
      "Precision : 86.43\n",
      "Recall : 79.42\n",
      "F1-Score :81.25\n",
      "[[365284  13069]\n",
      " [ 82111 135630]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "import joblib\r\n",
    "\r\n",
    "# save model\r\n",
    "joblib_file_svm = \"model/bahasa_sentiment_svm_model2.pkl\"\r\n",
    "joblib.dump(svm_model, joblib_file_svm)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['model/bahasa_sentiment_svm_model2.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "# load model\r\n",
    "joblib_SVM_model = joblib.load(joblib_file_svm)\r\n",
    "sentiment_model_predict(joblib_SVM_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 94.7\n",
      "Precision : 94.04\n",
      "Recall : 94.66\n",
      "F1-Score :94.33\n",
      "[[358735  19618]\n",
      " [ 11949 205792]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "# joblib_file_lr = \"model/bahasa_sentiment_lr_model2.pkl\"\r\n",
    "# joblib.dump(lr_model, joblib_file_lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# joblib_LR_model = joblib.load(joblib_file_lr)\r\n",
    "# sentiment_model_predict(joblib_LR_model,X_test,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "b3f142a98a6d23ed969ea10671f9b46be818a2fb2bf860b2d6208d7344d8ce18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}