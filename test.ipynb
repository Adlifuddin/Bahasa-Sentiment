{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "import nltk\r\n",
    "# 1: +ve, 0: -ve"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import re\r\n",
    "import stopwordsiso as sw\r\n",
    "\r\n",
    "# download required library from nltk\r\n",
    "# nltk.download('punkt')\r\n",
    "\r\n",
    "# create stemmer for bahasa\r\n",
    "factory = StemmerFactory()\r\n",
    "stemmer = factory.create_stemmer()\r\n",
    "\r\n",
    "# stop words consist of malay, indo, english\r\n",
    "stop_words_main = list(sw.stopwords([\"ms\", \"id\", \"en\"]))\r\n",
    "# custom stopwords such as shortform\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words_main+stop_words_custom)\r\n",
    "\r\n",
    "def remove_emoji(text):\r\n",
    "    emoji_pattern = re.compile(\"[\"\r\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\r\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\r\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\r\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\r\n",
    "                           u\"\\U00002702-\\U000027B0\"\r\n",
    "                           u\"\\U000024C2-\\U0001F251\"\r\n",
    "                           \"]+\", flags=re.UNICODE)\r\n",
    "    return emoji_pattern.sub(r'', text)\r\n",
    "\r\n",
    "def text_preprocessing(text):\r\n",
    "\r\n",
    "    # remove numbers\r\n",
    "    text = re.sub(r'\\d+', '', text)\r\n",
    "\r\n",
    "    # remove links\r\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "\r\n",
    "    # remove word with tweethandle @name\r\n",
    "    text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "\r\n",
    "    # remove emoji\r\n",
    "    text = remove_emoji(text)\r\n",
    "\r\n",
    "    # tokennization\r\n",
    "    tokens = word_tokenize(text)\r\n",
    "\r\n",
    "    # stemmer and remove punctuation\r\n",
    "    words = []\r\n",
    "    for token in tokens:\r\n",
    "        if token not in string.punctuation:\r\n",
    "            temp = stemmer.stem(token)\r\n",
    "            words.append(temp)\r\n",
    "\r\n",
    "    # remove stopwords\r\n",
    "    cleaned = []\r\n",
    "    for word in words:\r\n",
    "        if word not in STOP_WORDS:\r\n",
    "            cleaned.append(word)\r\n",
    "\r\n",
    "    # join all words into a complete sentence \r\n",
    "    complete_sentence = ' '.join([str(word) for word in cleaned])\r\n",
    "\r\n",
    "    # remove extra line spaces between words in a sentence\r\n",
    "    complete_sentence = \" \".join(complete_sentence.split())\r\n",
    "    \r\n",
    "    return complete_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "test_data1 = pd.read_csv('dataset/news-test-data.csv')\r\n",
    "test_data1 = test_data1[test_data1.label != 'Neutral']\r\n",
    "test_data1.loc[test_data1['label'] == 'Positive', 'label'] = 1\r\n",
    "test_data1.loc[test_data1['label'] == 'Negative', 'label'] = 0\r\n",
    "test_data1.loc[test_data1['label'] == 'negative', 'label'] = 0\r\n",
    "test_data1['label'] = test_data1['label'].astype('int64')\r\n",
    "test_data1 = test_data1[['text', 'label']]\r\n",
    "print(test_data1.head(10))\r\n",
    "print(len(test_data1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                 text  label\n",
      "0   Permohonan ini juga bertujuan memohon penjelas...      1\n",
      "1   Projek ini bukti komitmen berterusan kerajaan ...      1\n",
      "2   Menerusi pembentangan Bajet 2018 yang lalu, ke...      1\n",
      "4   Pada masa sama, kerajaan negeri juga telah bua...      1\n",
      "5   Kita bawa pembangunan, bukan sahaja untuk Saba...      1\n",
      "6   Harapan kita agar isu berkaitan golongan muda ...      1\n",
      "8   Kalau dah bersara dan tiada kaitan lagi dengan...      1\n",
      "9   Kesediaan negara-negara ITRC untuk mematuhi ko...      1\n",
      "10  Apa yang menjadi masalah kita, apabila masyara...      0\n",
      "12  Jabatan Kehakiman (DOJ) Amerika Syarikat sebel...      0\n",
      "2942\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "text_cleaning = lambda x: text_preprocessing(x)\r\n",
    "test_data1['cleaned_Text'] = pd.DataFrame(test_data1['text'].apply(text_cleaning))\r\n",
    "test_data1.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Permohonan ini juga bertujuan memohon penjelas...</td>\n",
       "      <td>1</td>\n",
       "      <td>mohon tuju mohon crcc pegang tuju khas spv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Projek ini bukti komitmen berterusan kerajaan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>bukti komitmen raja baris sekutu jaga kebaji m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Menerusi pembentangan Bajet 2018 yang lalu, ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>terusi bentang bajet raja sedia promosi tingka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pada masa sama, kerajaan negeri juga telah bua...</td>\n",
       "      <td>1</td>\n",
       "      <td>raja bayar rm mangsa banjir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kita bawa pembangunan, bukan sahaja untuk Saba...</td>\n",
       "      <td>1</td>\n",
       "      <td>bawa bangun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harapan kita agar isu berkaitan golongan muda ...</td>\n",
       "      <td>1</td>\n",
       "      <td>harap kait golong muda didik perhati manifesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kalau dah bersara dan tiada kaitan lagi dengan...</td>\n",
       "      <td>1</td>\n",
       "      <td>sara kait felda tindak sivil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kesediaan negara-negara ITRC untuk mematuhi ko...</td>\n",
       "      <td>1</td>\n",
       "      <td>sedia itrc patuh kouta mekanisme impak positif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Apa yang menjadi masalah kita, apabila masyara...</td>\n",
       "      <td>0</td>\n",
       "      <td>gemar kongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jabatan Kehakiman (DOJ) Amerika Syarikat sebel...</td>\n",
       "      <td>0</td>\n",
       "      <td>jabat hakim doj dakwa aset mewah beli curi mdb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  \\\n",
       "0   Permohonan ini juga bertujuan memohon penjelas...      1   \n",
       "1   Projek ini bukti komitmen berterusan kerajaan ...      1   \n",
       "2   Menerusi pembentangan Bajet 2018 yang lalu, ke...      1   \n",
       "4   Pada masa sama, kerajaan negeri juga telah bua...      1   \n",
       "5   Kita bawa pembangunan, bukan sahaja untuk Saba...      1   \n",
       "6   Harapan kita agar isu berkaitan golongan muda ...      1   \n",
       "8   Kalau dah bersara dan tiada kaitan lagi dengan...      1   \n",
       "9   Kesediaan negara-negara ITRC untuk mematuhi ko...      1   \n",
       "10  Apa yang menjadi masalah kita, apabila masyara...      0   \n",
       "12  Jabatan Kehakiman (DOJ) Amerika Syarikat sebel...      0   \n",
       "\n",
       "                                         cleaned_Text  \n",
       "0          mohon tuju mohon crcc pegang tuju khas spv  \n",
       "1   bukti komitmen raja baris sekutu jaga kebaji m...  \n",
       "2   terusi bentang bajet raja sedia promosi tingka...  \n",
       "4                         raja bayar rm mangsa banjir  \n",
       "5                                         bawa bangun  \n",
       "6      harap kait golong muda didik perhati manifesto  \n",
       "8                        sara kait felda tindak sivil  \n",
       "9   sedia itrc patuh kouta mekanisme impak positif...  \n",
       "10                                       gemar kongsi  \n",
       "12     jabat hakim doj dakwa aset mewah beli curi mdb  "
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import joblib\r\n",
    "\r\n",
    "X_test = test_data1['cleaned_Text']\r\n",
    "y_test = test_data1['label']\r\n",
    "\r\n",
    "# load model\r\n",
    "joblib_SVM_model2 = joblib.load(\"model/bahasa_sentiment_svm_model2.pkl\")\r\n",
    "sentiment_model_predict(joblib_SVM_model2,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 61.01\n",
      "Precision : 64.84\n",
      "Recall : 67.02\n",
      "F1-Score :60.54\n",
      "[[ 736  158]\n",
      " [ 989 1059]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import json\r\n",
    "\r\n",
    "with open('dataset/media-test-data-negatives.json') as fopen:\r\n",
    "    file_test = json.load(fopen)\r\n",
    "\r\n",
    "test_data2 = pd.DataFrame(file_test, columns = ['text'])\r\n",
    "# negative_data = negative_data[:100000]\r\n",
    "# test_data2['label'] = 0\r\n",
    "test_data2 = test_data2.head(3000)\r\n",
    "print(test_data2.head(10))\r\n",
    "print(len(test_data2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                text\n",
      "0  @Iwwannnnn apa lancau mintak izin kat awek . a...\n",
      "1  Mas Bowo di marahin tukang parkir!\\n\\n\"Ancene ...\n",
      "2      Ala bodohnya jilat. Gersang sgt ka. Geli aku \n",
      "3  @lalajoeee Si lancau ni mmg kuat cari pasal. B...\n",
      "4  Eyh babila\\nDah pandai masok umah tu tutup la ...\n",
      "5  Ilmu gratis tapi bukan berarti boleh ngebodoh ...\n",
      "6  Makin melampo mintak murah !! Butoh !! Dah nak...\n",
      "7  @ainohyeah Amboi marahnya.  Tapi durian tu nam...\n",
      "8  SI ADAM NI NAFSU KUDA AKU RASA?!!! HA RETI PUL...\n",
      "9  Bodohlah. Janji lain, buat lain. Aku dah lah b...\n",
      "3000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "text_cleaning = lambda x: text_preprocessing(x)\r\n",
    "test_data2['cleaned_Text'] = pd.DataFrame(test_data2['text'].apply(text_cleaning))\r\n",
    "test_data2.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Iwwannnnn apa lancau mintak izin kat awek . a...</td>\n",
       "      <td>lancau mintak izin awek mak halang bini perempuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mas Bowo di marahin tukang parkir!\\n\\n\"Ancene ...</td>\n",
       "      <td>mas bowo marahin tukang parkir ancene capres c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ala bodohnya jilat. Gersang sgt ka. Geli aku</td>\n",
       "      <td>ala bodoh jilat gersang sgt ka geli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@lalajoeee Si lancau ni mmg kuat cari pasal. B...</td>\n",
       "      <td>lancau mmg kuat cari pasal ade cari pasal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eyh babila\\nDah pandai masok umah tu tutup la ...</td>\n",
       "      <td>eyh babila pandai masok umah tutup pintu shibe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ilmu gratis tapi bukan berarti boleh ngebodoh ...</td>\n",
       "      <td>ilmu gratis arti ngebodoh bodohin nih gym bawa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Makin melampo mintak murah !! Butoh !! Dah nak...</td>\n",
       "      <td>melampo mintak murah butoh mcam trade kt kedai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@ainohyeah Amboi marahnya.  Tapi durian tu nam...</td>\n",
       "      <td>amboi marah durian nampak sedap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SI ADAM NI NAFSU KUDA AKU RASA?!!! HA RETI PUL...</td>\n",
       "      <td>adam nafsu kuda ha ret pulak allahuakhbar babi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bodohlah. Janji lain, buat lain. Aku dah lah b...</td>\n",
       "      <td>bodoh janji takde ragam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @Iwwannnnn apa lancau mintak izin kat awek . a...   \n",
       "1  Mas Bowo di marahin tukang parkir!\\n\\n\"Ancene ...   \n",
       "2      Ala bodohnya jilat. Gersang sgt ka. Geli aku    \n",
       "3  @lalajoeee Si lancau ni mmg kuat cari pasal. B...   \n",
       "4  Eyh babila\\nDah pandai masok umah tu tutup la ...   \n",
       "5  Ilmu gratis tapi bukan berarti boleh ngebodoh ...   \n",
       "6  Makin melampo mintak murah !! Butoh !! Dah nak...   \n",
       "7  @ainohyeah Amboi marahnya.  Tapi durian tu nam...   \n",
       "8  SI ADAM NI NAFSU KUDA AKU RASA?!!! HA RETI PUL...   \n",
       "9  Bodohlah. Janji lain, buat lain. Aku dah lah b...   \n",
       "\n",
       "                                        cleaned_Text  \n",
       "0  lancau mintak izin awek mak halang bini perempuan  \n",
       "1  mas bowo marahin tukang parkir ancene capres c...  \n",
       "2                ala bodoh jilat gersang sgt ka geli  \n",
       "3          lancau mmg kuat cari pasal ade cari pasal  \n",
       "4  eyh babila pandai masok umah tutup pintu shibe...  \n",
       "5  ilmu gratis arti ngebodoh bodohin nih gym bawa...  \n",
       "6  melampo mintak murah butoh mcam trade kt kedai...  \n",
       "7                    amboi marah durian nampak sedap  \n",
       "8  adam nafsu kuda ha ret pulak allahuakhbar babi...  \n",
       "9                            bodoh janji takde ragam  "
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import joblib\r\n",
    "\r\n",
    "# load model\r\n",
    "joblib_SVM_model2 = joblib.load(\"model/bahasa_sentiment_svm_model2.pkl\")\r\n",
    "pred_data=joblib_SVM_model2.predict(test_data2['cleaned_Text'])\r\n",
    "test_data2['Predicted'] = pred_data\r\n",
    "test_data2['Predicted'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    2317\n",
       "1     683\n",
       "Name: Predicted, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "b3f142a98a6d23ed969ea10671f9b46be818a2fb2bf860b2d6208d7344d8ce18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}